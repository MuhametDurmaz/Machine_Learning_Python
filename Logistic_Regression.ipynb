{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Logistic Regression\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "\n",
    "\n",
    "2.1 Problem Statement\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicantâ€™s chance of admission based on their results on two exams.\n",
    "\n",
    "You have historical data from previous applicants that you can use as a training set for logistic regression.\n",
    "For each training example, you have the applicantâ€™s scores on two exams and the admissions decision.\n",
    "Your task is to build a classification model that estimates an applicantâ€™s probability of admission based on the scores from those two exams.\n",
    "\n",
    "2.2 Loading and visualizing the data\n",
    "You will start by loading the dataset for this task.\n",
    "\n",
    "The load_dataset() function shown below loads the data into variables X_train and y_train\n",
    "X_train contains exam scores on two exams for a student\n",
    "y_train is the admission decision\n",
    "y_train = 1 if the student was admitted\n",
    "y_train = 0 if the student was not admitted\n",
    "Both X_train and y_train are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X_train, y_train = load_data(\"data/ex2data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Sigmoid function\n",
    "Recall that for logistic regression, the model is represented as\n",
    "\n",
    "ğ‘“ğ°,ğ‘(ğ‘¥)=ğ‘”(ğ°â‹…ğ±+ğ‘)\n",
    " \n",
    "where function  ğ‘”  is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§\n",
    " \n",
    "Let's implement the sigmoid function first, so it can be used by the rest of this assignment.\n",
    "\n",
    "\n",
    "Exercise 1\n",
    "Please complete the sigmoid function to calculate\n",
    "\n",
    "ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§\n",
    " \n",
    "Note that\n",
    "\n",
    "z is not always a single number, but can also be an array of numbers.\n",
    "If the input is an array of numbers, we'd like to apply the sigmoid function to each value in the input array.\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for logistic function (sigmoid function)\n",
    "def sigmoid(z):\n",
    "  \n",
    "    g = 1/(1+np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the cost\n",
    "def compute_cost(X, y, w, b,lambda_=1):\n",
    "    m, n = X.shape\n",
    "\n",
    "    total_cost=0\n",
    "    for i in range(m):\n",
    "        z=np.dot(w,X[i,:])+b# activation function\n",
    "        f=sigmoid(z)# sigmoid function\n",
    "        loss = (-y[i]*np.log(f))-(1-y[i])*np.log(1-f)#loss\n",
    "        total_cost+=loss\n",
    "    total_cost=total_cost/m\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for optimizing the parameters using gradiet descent\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "\n",
    "    m = len(X)\n",
    "\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for predicting the values using decision boundary as 0.5\n",
    "def predict(X, w, b): \n",
    "\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    for i in range(m):   \n",
    "        z_wb = 0\n",
    "        for j in range(n): \n",
    "            z_wb += w[j]*X[i,j]\n",
    "        z_wb += b\n",
    "\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        if f_wb >= 0.5:\n",
    "            p[i] = 1.0\n",
    "        else:\n",
    "            p[i] = 0.0\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Regularized Logistic Regression\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "\n",
    "\n",
    "3.1 Problem Statement\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests.\n",
    "\n",
    "From these two tests, you would like to determine whether the microchips should be accepted or rejected.\n",
    "To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n",
    "\n",
    "3.2 Loading and visualizing the data\n",
    "Similar to previous parts of this exercise, let's start by loading the dataset for this task and visualizing it.\n",
    "\n",
    "The load_dataset() function shown below loads the data into variables X_train and y_train\n",
    "X_train contains the test results for the microchips from two tests\n",
    "y_train contains the results of the QA\n",
    "y_train = 1 if the microchip was accepted\n",
    "y_train = 0 if the microchip was rejected\n",
    "Both X_train and y_train are numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the feature mapping allows us to build a more expressive classifier, it is also more susceptible to overfitting. In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n",
    "\n",
    "\n",
    "3.4 Cost function for regularized logistic regression\n",
    "In this part, you will implement the cost function for regularized logistic regression.\n",
    "\n",
    "Recall that for regularized logistic regression, the cost function is of the form\n",
    "ğ½(ğ°,ğ‘)=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1[âˆ’ğ‘¦(ğ‘–)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))]+ğœ†2ğ‘šâˆ‘ğ‘—=0ğ‘›âˆ’1ğ‘¤2ğ‘—\n",
    " \n",
    "Compare this to the cost function without regularization (which you implemented above), which is of the form\n",
    "\n",
    "ğ½(ğ°.ğ‘)=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1[(âˆ’ğ‘¦(ğ‘–)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))]\n",
    " \n",
    "The difference is the regularization term, which is\n",
    "ğœ†2ğ‘šâˆ‘ğ‘—=0ğ‘›âˆ’1ğ‘¤2ğ‘—\n",
    " \n",
    "Note that the  ğ‘  parameter is not regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating cost with regularization\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    m, n = X.shape\n",
    "\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    reg_cost = 0.\n",
    "    for j in range(n):\n",
    "        reg_cost+=w[j]**2\n",
    "\n",
    "    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost #regularization\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Gradient for regularized logistic regression\n",
    "In this section, you will implement the gradient for regularized logistic regression.\n",
    "\n",
    "The gradient of the regularized cost function has two components. The first,  âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘  is a scalar, the other is a vector with the same shape as the parameters  ğ° , where the  ğ‘—th  element is defined as follows:\n",
    "\n",
    "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))\n",
    " \n",
    "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—=(1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)ğ‘—)+ğœ†ğ‘šğ‘¤ğ‘—for ğ‘—=0...(ğ‘›âˆ’1)\n",
    " \n",
    "Compare this to the gradient of the cost function without regularization (which you implemented above), which is of the form\n",
    "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ²(ğ‘–))(2)\n",
    "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ²(ğ‘–))ğ‘¥(ğ‘–)ğ‘—(3)\n",
    "As you can see, âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘  is the same, the difference is the following term in  âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ , which is\n",
    "ğœ†ğ‘šğ‘¤ğ‘—for ğ‘—=0...(ğ‘›âˆ’1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating regularized gradient\n",
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "    for j in range(n):\n",
    "        dj_dw[j]+=lambda_/m*w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
